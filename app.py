#!/usr/bin/env python3
"""Clean Streamlit app for Elite-K phone review analyzer.

This file contains a single consistent implementation and no duplicated
blocks. It imports helpers from src.app_helpers for CSV normalization.
"""

# IMPORTANT: Set this BEFORE importing any HuggingFace libraries
# to suppress subprocess fork warnings when using Ollama CLI
import os
os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')

from pathlib import Path
import sys
ROOT = Path(__file__).resolve().parent.parent
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

import streamlit as st
import pandas as pd
import re
from typing import Tuple, List, Dict
import difflib

from src.config import DEFAULT_CONFIG
from src import sentiment_engine
from src.api_clients import HFClient, MockHFClient
from src.fake_detector import detect_fake_reviews
from src.sentiment_engine import analyze_sentiments
from src.summarizer import summarize_reviews
from src.video_pipeline import analyze_video, fuse_scores
from src.adapters import get_adapter
from src.score_phone import compute_trust_score
from src.app_helpers import ensure_review_columns, ensure_product_column
from src.aspect_extractor import map_sentences_to_reviews

st.set_page_config(
    page_title="Elite-K Phone Review Analyzer", 
    layout="wide",
    initial_sidebar_state="expanded"  # Open sidebar by default
)
st.title("Elite-K â€” Smartphone Review Analyzer (MVP)")

# initialize session state keys used for UI persistence
for k, v in {
    "last_results": None,
    "last_df_rows": None,
    "pinned_product": None,
    "selected_product": None,
    "sort_field": "final_score",
    "sort_dir": "Descending",
}.items():
    if k not in st.session_state:
        st.session_state[k] = v

    # ensure HF client selection is preserved in session state for reproducible runs
    if 'use_hf' not in st.session_state:
        st.session_state['use_hf'] = False

def detect_price_column(df: pd.DataFrame) -> Tuple[pd.DataFrame, str]:
    """Detect a price column name heuristically. Returns (df, price_col_or_None)."""
    lower = {c.lower(): c for c in df.columns}
    candidates = ["price", "cost", "amount", "price_usd", "retail_price", "msrp"]
    for cand in candidates:
        if cand in lower:
            df = df.rename(columns={lower[cand]: "price"})
            # try to coerce the detected column to float
            df["price"] = df["price"].apply(_parse_price)
            return df, "price"
    # fallback: numeric column with plausible price range
    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    for c in num_cols:
        vals = df[c].dropna()
        if len(vals) == 0:
            continue
        # check percent of values within 0..1500 USD
        within = ((vals >= 0) & (vals <= 1500)).sum() / len(vals)
        if within > 0.6:
            df = df.rename(columns={c: "price"})
            df["price"] = df["price"].astype(float)
            return df, "price"
    # try to find string columns that look like currency (e.g., "$199", "199.99", "1,299", "199 USD")
    str_cols = [c for c in df.columns if not pd.api.types.is_numeric_dtype(df[c])]
    for c in str_cols:
        sample = df[c].dropna().astype(str).head(200)
        if len(sample) == 0:
            continue
        parsed = sample.map(_parse_price)
        success = parsed.notna().sum() / len(parsed)
        if success > 0.6:
            # coerce entire column
            df = df.rename(columns={c: "price"})
            df["price"] = df["price"].map(_parse_price)
            # check plausible range
            vals = df["price"].dropna()
            if len(vals) > 0 and ((vals >= 0) & (vals <= 1500)).sum() / len(vals) > 0.5:
                return df, "price"
            # else undo and continue
            df = df.rename(columns={"price": c})
    return df, None


def _parse_price(x):
    """Parse a price-like value into float, stripping currency symbols and commas. Returns NaN on failure."""
    try:
        if pd.isna(x):
            return float("nan")
    except Exception:
        pass
    s = str(x).strip()
    # remove common currency symbols and letters
    s = re.sub(r"[\$,Â£â‚¬]", "", s)
    s = re.sub(r"(?i)\s*usd\b", "", s)
    s = s.replace(',', '')
    s = s.replace('\u00a0', '')
    s = s.strip()
    # allow values like '199', '199.99', '-'
    if s in ['', '-', 'â€“', 'â€”']:
        return float("nan")
    # extract first numeric occurrence
    m = re.search(r"-?\d{1,3}(?:\d{3})*(?:[\.,]\d+)?|-?\d+(?:[\.,]\d+)?", s)
    if not m:
        return float("nan")
    num = m.group(0)
    num = num.replace(',', '')
    num = num.replace(' ', '')
    try:
        return float(num)
    except Exception:
        try:
            return float(num.replace(',', ''))
        except Exception:
            return float("nan")


def normalize_model_name(name: str) -> str:
    """Normalize model string to a base model name used for grouping.

    Removes memory/storage tokens and parentheses, trims whitespace.
    """
    if name is None:
        return ""
    s = str(name)
    # remove parentheses
    s = re.sub(r"\(.*?\)", "", s)
    # remove RAM/storage tokens like '6GB RAM', '8 GB', '128GB'
    s = re.sub(r"\b\d+\s*(gb|mb|tb)\b", "", s, flags=re.IGNORECASE)
    s = re.sub(r"\bram\b", "", s, flags=re.IGNORECASE)
    # remove 'with' clauses and hyphenated trailing specs
    s = re.sub(r"with\b.*", "", s, flags=re.IGNORECASE)
    s = re.sub(r"\b1280p|1080p|4k|5g\b", "", s, flags=re.IGNORECASE)
    s = re.sub(r"[-â€“â€”].*$", "", s)
    # collapse multiple spaces and strip
    s = re.sub(r"\s+", " ", s).strip()
    return s


def group_models(names: List[str], cutoff: float = 0.85) -> Dict[str, List[str]]:
    """Group variant model names under a base normalized model.

    Returns mapping: base_name -> list of original variants
    """
    norm_map = {n: normalize_model_name(n) or n for n in names}
    bases = {}
    used = set()
    unique_norms = list({v for v in norm_map.values()})
    for norm in unique_norms:
        bases[norm] = []
    # fuzzy match originals to norms
    for orig, norm in norm_map.items():
        # use difflib to find closest norm (including itself)
        choices = difflib.get_close_matches(norm, unique_norms, n=1, cutoff=cutoff)
        key = choices[0] if choices else norm
        bases.setdefault(key, []).append(orig)
    # remove empty keys
    return {k: sorted(set(v)) for k, v in bases.items() if v}


def detect_brands(df: pd.DataFrame, product_col: str) -> Tuple[pd.DataFrame, List[str]]:
    """Attempt to infer a `brand` column from the `product` values.

    Returns (df_with_brand_col, sorted_brand_list).
    """
    known = ["Samsung", "Apple", "Xiaomi", "OnePlus", "Oppo", "Vivo", "Google", "Sony", "Motorola", "Nokia", "Realme", "Huawei", "LG", "HTC"]
    brands = set()

    def infer_brand(prod):
        s = str(prod or "").strip()
        if not s:
            return "(unknown)"
        for b in known:
            if re.search(rf"\b{re.escape(b)}\b", s, flags=re.I):
                brands.add(b)
                return b
        # fallback: first token (capitalized) or whole first word
        first = s.split()[0]
        brands.add(first)
        return first

    df = df.copy()
    if product_col in df.columns:
        # store brands normalized to lowercase to avoid case-sensitivity issues
        df["brand"] = df[product_col].fillna("").astype(str).map(lambda p: infer_brand(p).lower())
    else:
        df["brand"] = "(unknown)"
        brands.add("(unknown)")
    return df, sorted({b.title() for b in brands})


def render_phone_table(df: pd.DataFrame, results: Dict[str, Dict]):
    """No-op: phones list section intentionally removed (redundant with the interactive table)."""
    return


def show_product_details(name: str, result: Dict):
    """Show full details for a product result in an expander."""
    with st.expander(f"Details â€” {name}", expanded=True):
        st.markdown(f"**Final score:** {result.get('trust', {}).get('final_score')}")
        st.markdown(f"**Confidence:** {result.get('trust', {}).get('confidence')}")
        st.markdown(f"**Fake reviews:** {result.get('fake_count')} of {result.get('n_total')} ({result.get('fake_pct'):.1f}%)")
        st.subheader("Balanced summary")
        st.write(result.get('summ', {}).get('balanced_summary'))
        st.subheader("Aspect breakdown")
        st.json(result.get('summ', {}).get('aspect_breakdown'))
        st.subheader("Trust details")
        st.json(result.get('trust'))



# Sidebar / runtime configuration
st.sidebar.header("Settings")

# Provider selection (moved before mock_mode so it can auto-set the mode)
st.sidebar.subheader("Sentiment Provider")
sentiment_provider = st.sidebar.radio(
    "Choose sentiment analysis provider:",
    ["Mock (fast/deterministic)", "Hugging Face (remote API)", "Ollama (local LLM)"],
    index=0,
    key="sentiment_provider_radio"
)

# Map display name to internal provider key
provider_map = {
    "Mock (fast/deterministic)": "mock",
    "Hugging Face (remote API)": "huggingface",
    "Ollama (local LLM)": "ollama"
}
selected_provider = provider_map[sentiment_provider]

# Auto-set mock_mode based on provider selection
# Mock provider -> mock_mode=True
# HuggingFace/Ollama -> mock_mode=False (use real APIs)
auto_mock_mode = (selected_provider == "mock")
mock_mode = st.sidebar.checkbox(
    "Mock mode for fake detection & summarization", 
    value=auto_mock_mode,
    help="When enabled, uses fast mock models for fake detection and summarization. Sentiment analysis is controlled by the provider above."
)

# Clear cached results when provider or mock_mode changes
current_settings = f"{selected_provider}_{mock_mode}"
if st.session_state.get('last_settings') != current_settings:
    st.session_state['last_settings'] = current_settings
    if 'last_results' in st.session_state:
        del st.session_state['last_results']
    if 'last_df_rows' in st.session_state:
        del st.session_state['last_df_rows']
    # Show info that settings changed
    if st.session_state.get('settings_changed_once'):
        st.sidebar.info("âš ï¸ Settings changed! Click 'Analyze selection' to update results.")
    st.session_state['settings_changed_once'] = True

adapter = get_adapter()
st.sidebar.markdown("Adapter: Smartphones (preloaded)")
rep_mode = st.sidebar.selectbox("Representative examples mode", ["Review-level (majority)", "Sentence-level (strict)"], index=0)

# Conditional inputs based on provider
hf_api_key = None
ollama_model = "llama3.2:3b"  # Default fallback

if selected_provider == "huggingface":
    hf_api_key = st.sidebar.text_input("Hugging Face API key (optional)", type="password", key="hf_api_key_input")
    if not hf_api_key and DEFAULT_CONFIG.get("mock_mode", True) is False:
        st.sidebar.warning("Hugging Face API selected but no key provided; will fall back to mock client.")
elif selected_provider == "ollama":
    # === CHANGED: Text input -> Selectbox for easy switching ===
    ollama_model = st.sidebar.selectbox(
        "Select Ollama model:",
        ["llama3.2:3b", "gemma2:2b", "gemma3:1b"],
        index=0,  # Default to llama3.2:3b
        key="ollama_model_select"
    )
    st.sidebar.info(f"Using: **{ollama_model}**")
    # Hint to install if not present
    st.sidebar.caption(f"Command to install: `ollama pull {ollama_model}`")

# Legacy HF checkbox support (kept for backward compatibility)
use_hf = st.sidebar.checkbox("Use Hugging Face Inference API", False, key="use_hf_legacy")
if use_hf:
    st.sidebar.warning("âš ï¸ Legacy 'Use HF' checkbox is deprecated. Please use the Provider selector above.")

if getattr(sentiment_engine, 'HF_MODELS_UNAVAILABLE', None):
    st.sidebar.warning("One or more Hugging Face models used by the app are unavailable; the app will use internal mock fallbacks for those models.")
if st.sidebar.checkbox("Show configuration (DEBUG)"):
    st.sidebar.json(DEFAULT_CONFIG)
    try:
        st.sidebar.write("Active Personal Weights:")
        st.sidebar.json(active_adapter["aspect_weights"])
    except Exception:
        st.sidebar.write("Adapter details not available")

tab = st.sidebar.selectbox("Action", ["Demo sample", "Upload CSV", "Analyze YouTube Video"]) 


def run_pipeline_on_df(sub_df: pd.DataFrame, max_reviews: int = 0, hf_client=None, provider="mock", ollama_model="llama3.2:3b", adapter=None):
    """Run the processing pipeline and return results dict.

    Results: { df1, sent, summ, trust }
    
    Args:
        provider: "mock", "huggingface", or "ollama"
        ollama_model: model name when using Ollama provider
        adapter: custom adapter configuration (optional)
    """
    if int(max_reviews) > 0 and int(max_reviews) < len(sub_df):
        sub_df = sub_df.head(int(max_reviews)).reset_index(drop=True)
        
    # Use global active_adapter if none provided
    if adapter is None:
        adapter = active_adapter # Use the personalized one from sidebar

    # choose HF client based on settings unless caller provided one
    if hf_client is None:
        hf_client = MockHFClient()
        if provider == "huggingface" and hf_api_key:
            hf_client = HFClient(api_key=hf_api_key)
        elif provider == "huggingface":
            # if HF selected and no explicit key, try HFClient with env var
            try:
                hf_client = HFClient()
            except Exception:
                hf_client = MockHFClient()

    # Pass the provider to detect_fake_reviews so it can use Ollama if selected
    df1 = detect_fake_reviews(
        sub_df, 
        hf_client=hf_client, 
        mock_mode=mock_mode, 
        provider=provider, 
        ollama_model=ollama_model
    )
    
    # For sentiment analysis, provider parameter controls behavior
    sent = analyze_sentiments(df1, hf_client=hf_client, provider=provider, ollama_model=ollama_model)
    df2 = sent["df"]
    
    genuine = df2[df2["is_fake"] == False]
    summ = summarize_reviews(genuine, hf_client=hf_client, mock_mode=mock_mode, provider=provider, ollama_model=ollama_model)
    
    # Compute Trust Score using the CUSTOM ADAPTER (Personalized Weights)
    summary = compute_trust_score(df2, adapter)
    return {"df1": df1, "sent": sent, "summ": summ, "trust": summary}


def _show_representative_reviews(sent_df: pd.DataFrame, label: str = "Representative reviews"):
    """Show one positive and one negative review from the provided sentiment df."""
    if sent_df is None or len(sent_df) == 0:
        st.write("No reviews available to show.")
        return
    # prefer genuine reviews if available
    if "is_fake" in sent_df.columns:
        genuine = sent_df[~sent_df["is_fake"]].copy()
    else:
        genuine = sent_df.copy()

    if genuine is None or len(genuine) == 0:
        genuine = sent_df.copy()

    # If configured, prefer review-level representative selection (reflects majority)
    if rep_mode.startswith("Review"):
        try:
            # filter genuine reviews and exclude mixed/weak ones
            good = genuine.copy()
            if "sentiment_label" in good.columns:
                good = good[~good["sentiment_label"].isin(["mixed"])].copy()
            # positive/negative candidates: split by compound sign
            pos_cands = good[(good.get("compound", 0.0) > 0)]
            neg_cands = good[(good.get("compound", 0.0) < 0)]
            chosen_pos = None
            chosen_neg = None
            if len(pos_cands) > 0:
                median_val = float(pos_cands["compound"].median())
                chosen_pos = pos_cands.iloc[(pos_cands["compound"].sub(median_val).abs()).argsort().iloc[0]]
            if len(neg_cands) > 0:
                median_val_n = float(neg_cands["compound"].median())
                chosen_neg = neg_cands.iloc[(neg_cands["compound"].sub(median_val_n).abs()).argsort().iloc[0]]
            
            if chosen_pos is not None or chosen_neg is not None:
                st.subheader(label)
                c1, c2 = st.columns(2)
                with c1:
                    st.markdown("**Positive (representative)**")
                    if chosen_pos is not None:
                        st.write(chosen_pos["reviewText"])
                        meta = {"compound": float(chosen_pos.get("compound", 0.0)), "strength": float(chosen_pos.get("strength", 0.0)), "hf_fake_score": float(chosen_pos.get("hf_fake_score", 0.0))}
                        if "rating" in chosen_pos.index:
                            meta["rating"] = float(chosen_pos.get("rating", None))
                        st.json(meta)
                    else:
                        st.write("No sufficiently positive representative found.")
                with c2:
                    st.markdown("**Negative (representative)**")
                    if chosen_neg is not None:
                        st.write(chosen_neg["reviewText"])
                        meta2 = {"compound": float(chosen_neg.get("compound", 0.0)), "strength": float(chosen_neg.get("strength", 0.0)), "hf_fake_score": float(chosen_neg.get("hf_fake_score", 0.0))}
                        if "rating" in chosen_neg.index:
                            meta2["rating"] = float(chosen_neg.get("rating", None))
                        st.json(meta2)
                    else:
                        st.write("No sufficiently negative representative found.")
                return
        except Exception:
            pass

    # Fallback to simple sorting
    if "compound" in genuine.columns:
        pos = genuine.sort_values("compound", ascending=False).iloc[0]
        neg = genuine.sort_values("compound", ascending=True).iloc[0]
    else:
        pos = genuine.iloc[0]
        neg = genuine.iloc[-1]

    st.subheader(label)
    col1, col2 = st.columns(2)
    with col1:
        st.markdown("**Positive example**")
        st.write(pos["reviewText"])
        meta = {"compound": float(pos.get("compound", 0.0)), "strength": float(pos.get("strength", 0.0))}
        st.json(meta)
    with col2:
        st.markdown("**Negative example**")
        st.write(neg["reviewText"])
        meta2 = {"compound": float(neg.get("compound", 0.0)), "strength": float(neg.get("strength", 0.0))}
        st.json(meta2)


if tab == "Demo sample":
    st.info("Using built-in phone sample dataset for demo.")
    df = pd.read_csv("data/phone_sample_reviews.csv")
    try:
        df = ensure_review_columns(df)
    except ValueError as err:
        st.error(str(err))
        st.stop()

    st.subheader("Demo: sample reviews")
    st.write(df.head())

    # Correct call to run_pipeline_on_df passing the active_adapter
    # For demo, we use default adapter unless user overrides in main UI (which is not shown for demo)
    default_adapter = get_adapter()
    out = run_pipeline_on_df(df, provider=selected_provider, ollama_model=ollama_model, adapter=default_adapter)
    trust = out["trust"]
    st.markdown("## Demo Final Trust Score")
    try:
        st.metric("Final Trust Score", f"{trust['final_score']:.1f}")
    except Exception:
        st.write("Final Trust Score:", trust.get("final_score"))
    st.write("Confidence:", trust.get("confidence"))
    st.markdown("---")

    st.subheader("Sentiment summary")
    st.json(out["sent"]["summary"])

    st.subheader("Aspect breakdown & summaries")
    st.json(out["summ"].get("aspect_breakdown"))


elif tab == "Upload CSV":
    uploaded = st.file_uploader("Upload reviews CSV (must have reviewText or similar)", type=["csv"])
    if not uploaded:
        st.info("Upload a CSV with reviews. The app will try to detect product/model column automatically.")
    else:
        if "uploaded_filename" not in st.session_state or st.session_state.get("uploaded_filename") != uploaded.name:
            st.session_state["uploaded_df"] = pd.read_csv(uploaded)
            st.session_state["uploaded_filename"] = uploaded.name
        df = st.session_state.get("uploaded_df")

        try:
            df = ensure_review_columns(df)
        except ValueError as err:
            st.error(str(err) + " â€” please upload a CSV with a review text column.")
            st.stop()

        df, product_col = ensure_product_column(df)
        st.write(df.head())
        st.markdown("---")
        st.info("Step 1: (optional) choose price range, product grouping and number of rows (n) to analyze.")
        
        # Price detection and filtering logic
        df, price_col = detect_price_column(df)
        if price_col:
            vals = df[price_col].dropna().astype(float)
            pmin = float(vals.min()) if not vals.empty else 0.0
            pmax = float(vals.max()) if not vals.empty else 1500.0
            pmin = max(0.0, min(1500.0, pmin))
            pmax = max(0.0, min(1500.0, pmax))
            col1, col2 = st.columns(2)
            with col1:
                min_price = st.number_input("Min Price (USD)", min_value=0.0, max_value=1500.0, value=pmin, step=1.0, format="%.2f")
            with col2:
                max_price = st.number_input("Max Price (USD)", min_value=0.0, max_value=1500.0, value=pmax, step=1.0, format="%.2f")
            
            if min_price > max_price:
                st.error("Min Price must be <= Max Price")
                st.stop()
            
            try:
                df = df[df[price_col].notna() & (df[price_col].astype(float) >= float(min_price)) & (df[price_col].astype(float) <= float(max_price))].reset_index(drop=True)
            except Exception:
                df[price_col] = df[price_col].map(lambda x: _parse_price(x))
                df = df[df[price_col].notna() & (df[price_col] >= float(min_price)) & (df[price_col] <= float(max_price))].reset_index(drop=True)
            if df.empty:
                st.warning("No phones found in the selected price range. Adjust the range or upload a different dataset.")
                st.stop()

        # Brand detection and filtering
        if product_col is not None:
            df, brand_list = detect_brands(df, product_col)
            brand_choice = st.selectbox("Filter by brand", ["All brands"] + brand_list)
            if brand_choice and brand_choice != "All brands":
                df = df[df["brand"].fillna("").str.lower() == brand_choice.lower()].reset_index(drop=True)
                if df.empty:
                    st.warning("No phones found for selected brand in this selection.")
                    st.stop()

        max_reviews = st.number_input("Max reviews to analyze per selection (0 = all)", min_value=0, value=0, step=1)

        # Product grouping
        products = []
        if product_col is not None:
            products = df[product_col].fillna("(unknown)").astype(str).unique().tolist()
            grouping_map = group_models(products)
            display_products = ["All (per-product)"] + list(grouping_map.keys())
            if len(display_products) > 1:
                choice = st.selectbox("Select product group (or 'All (per-product)')", display_products)
            else:
                choice = display_products[0]
        else:
            grouping_map = {}
            choice = None

        # === DYNAMIC PERSONALIZATION UI (MAIN AREA) ===
        st.markdown("---")
        st.subheader("ðŸŽ¯ Personalize Your Analysis")
        st.markdown("Rank your top priorities. The system will use this to weight the scores.")

        default_adapter = get_adapter()
        all_aspects = list(default_adapter["aspect_weights"].keys())
        # weights pool from high to low
        available_weights = sorted(default_adapter["aspect_weights"].values(), reverse=True)

        # Use multiselect for ordering
        user_priorities = st.multiselect(
            "Select priorities (ordered by importance):",
            options=all_aspects,
            default=None,
            help="First item = 18%, Second = 16%, etc. Unselected items get remaining low weights."
        )
        
        # Display current priorities horizontally like tags
        if user_priorities:
            st.markdown("**Your Priority Ranking:**")
            priority_html = ""
            for i, p in enumerate(user_priorities):
                if i < len(available_weights):
                    weight_pct = available_weights[i] * 100
                    priority_html += f"<span style='background-color: #e6f3ff; color: #0068c9; padding: 4px 8px; border-radius: 4px; margin-right: 8px; font-weight: 500;'>{i+1}. {p.title()} ({weight_pct:.0f}%)</span>"
                else:
                    # Should not happen if we have enough weights, but safe guard
                    priority_html += f"<span style='background-color: #f0f2f6; padding: 4px 8px; border-radius: 4px; margin-right: 8px;'>{i+1}. {p.title()}</span>"
            
            st.markdown(priority_html, unsafe_allow_html=True)
            
            # Add a "Reset" button functionality (Streamlit doesn't have a direct reset for multiselect, 
            # but we can clear the list in the UI by just deselecting)
            # st.caption("To reset, clear the selection box.")
            
            # Build custom adapter
            final_order = user_priorities + [a for a in all_aspects if a not in user_priorities]
            new_weights_map = dict(zip(final_order, available_weights))
            active_adapter = default_adapter.copy()
            active_adapter["aspect_weights"] = new_weights_map
            
            # Show the implicit/default priorities for the rest
            if len(user_priorities) < len(all_aspects):
                remaining = [a for a in all_aspects if a not in user_priorities]
                remaining_html = "<span style='color:gray; font-size:0.9em;'>Remaining (auto-assigned): "
                start_idx = len(user_priorities)
                for i, p in enumerate(remaining):
                    w_idx = start_idx + i
                    if w_idx < len(available_weights):
                        w_pct = available_weights[w_idx] * 100
                        remaining_html += f"{p.title()} ({w_pct:.0f}%), "
                remaining_html = remaining_html.rstrip(", ") + "</span>"
                st.markdown(remaining_html, unsafe_allow_html=True)

        else:
            st.info("Using default balanced priorities. Select aspects above to customize.")
            active_adapter = default_adapter
        
        st.markdown("---")
        # ==============================================

        analyze_btn = st.button("Analyze selection")

        # ensure hf_client exists
        hf_client = MockHFClient()
        if selected_provider == "huggingface" and hf_api_key:
            hf_client = HFClient(api_key=hf_api_key)
        elif selected_provider == "huggingface":
            try:
                hf_client = HFClient()
            except Exception:
                hf_client = MockHFClient()

        if analyze_btn:
            results = {}
            # Process each group
            for base, variants in grouping_map.items():
                sub = df[df[product_col].fillna("(unknown)").astype(str).isin(variants)].reset_index(drop=True)
                if len(sub) == 0:
                    continue
                if int(max_reviews) > 0:
                    sub = sub.head(int(max_reviews)).reset_index(drop=True)

                # --- MANUAL PIPELINE STEPS FOR PER-PRODUCT ANALYSIS ---
                # Step A: Fake detection (Passing provider explicitly)
                df1 = detect_fake_reviews(
                    sub, 
                    hf_client=hf_client, 
                    mock_mode=mock_mode, 
                    provider=selected_provider, 
                    ollama_model=ollama_model
                )
                fake_count = int(df1.get("is_fake", pd.Series([False] * len(df1))).sum())
                n_total = len(df1)
                fake_pct = 100.0 * fake_count / n_total if n_total > 0 else 0.0

                # Step B: Sentiment Analysis
                sent_res = analyze_sentiments(df1, hf_client=hf_client, provider=selected_provider, ollama_model=ollama_model)
                sent_df = sent_res["df"]

                # Step C: Summarize
                genuine = sent_df[~sent_df["is_fake"]].reset_index(drop=True) if "is_fake" in sent_df.columns else sent_df
                summ = summarize_reviews(genuine, hf_client=hf_client, mock_mode=mock_mode, provider=selected_provider, ollama_model=ollama_model)

                # Step D: Trust score (USING PERSONALIZED ADAPTER)
                trust = compute_trust_score(sent_df, active_adapter)

                results[base] = {"df1": df1, "sent": sent_res, "summ": summ, "trust": trust, "fake_count": fake_count, "fake_pct": fake_pct, "n_total": n_total}

            # Create summary rows
            rows = [
                {"product": p, "n_reviews": r.get("n_total"), "fake_count": r.get("fake_count"), "fake_pct": f"{r.get('fake_pct'):.1f}%", "final_score": r["trust"].get("final_score"), "confidence": r["trust"].get("confidence")} 
                for p, r in results.items()
            ]
            df_rows = pd.DataFrame(rows)
            st.session_state['last_results'] = results
            st.session_state['last_df_rows'] = df_rows
            st.session_state['last_analysis_provider'] = selected_provider
            st.session_state['last_analysis_mock_mode'] = mock_mode
            st.session_state['last_analysis_ollama_model'] = ollama_model if selected_provider == "ollama" else None
            # Store used adapter weights for display reference
            st.session_state['last_analysis_adapter_weights'] = active_adapter["aspect_weights"]

        # Display results logic
        if st.session_state.get('last_results'):
            results = st.session_state['last_results']
            df_rows = st.session_state.get('last_df_rows', pd.DataFrame())
            used_weights = st.session_state.get('last_analysis_adapter_weights', default_adapter["aspect_weights"])
            
            # Show active settings
            analysis_provider = st.session_state.get('last_analysis_provider', 'unknown')
            analysis_mock_mode = st.session_state.get('last_analysis_mock_mode', True)
            analysis_ollama_model = st.session_state.get('last_analysis_ollama_model')
            
            provider_display = {
                "mock": "Mock (fast/deterministic)",
                "huggingface": "Hugging Face (remote API)",
                "ollama": f"Ollama ({analysis_ollama_model})" if analysis_ollama_model else "Ollama (local LLM)"
            }
            
            st.info(f"ðŸ“Š **Analysis Results** | Sentiment Provider: **{provider_display.get(analysis_provider, analysis_provider)}** | Fake Detection: **{'Mock Mode' if analysis_mock_mode else 'Real Mode'}**")

            if choice == "All (per-product)":
                st.markdown("**Sort results (interactive)**")
                sort_field = st.selectbox("Sort by", ["final_score", "n_reviews", "fake_count", "fake_pct", "confidence"], index=0, key="sort_field")
                sort_dir = st.radio("Order", ["Descending", "Ascending"], index=0, horizontal=True, key="sort_dir")
                asc = sort_dir == "Ascending"

                tmp = df_rows.copy()
                if sort_field == "fake_pct":
                    tmp["_fake_pct_num"] = tmp["fake_pct"].str.replace("%", "").astype(float)
                    df_sorted = tmp.sort_values(by=["_fake_pct_num"], ascending=asc)
                else:
                    df_sorted = tmp.sort_values(by=[sort_field], ascending=asc, na_position='last')

                st.table(df_sorted.reset_index(drop=True))

                # Interactive table actions
                scores = [(r_name, r_data.get("trust", {}).get("final_score")) for r_name, r_data in results.items()]
                scores = [(n, (s if s is not None else float("-inf"))) for n, s in scores]
                if scores:
                    max_prod, max_score = max(scores, key=lambda t: (t[1] is not None, t[1]))
                    min_prod, min_score = min(scores, key=lambda t: (t[1] is not None, t[1]))
                    col1, col2 = st.columns(2)
                    with col1:
                        st.markdown(f"**Max score:** {max_score if max_score not in (None, float('-inf')) else 'N/A'} â€” {max_prod}")
                        if st.button(f"View max", key="view_max_button"):
                            st.session_state['pinned_product'] = max_prod
                            st.session_state['selected_product'] = max_prod
                    with col2:
                        st.markdown(f"**Min score:** {min_score if min_score not in (None, float('-inf')) else 'N/A'} â€” {min_prod}")
                        if st.button(f"View min", key="view_min_button"):
                            st.session_state['pinned_product'] = min_prod
                            st.session_state['selected_product'] = min_prod

                # Expanders for details
                pinned = st.session_state.get('pinned_product')
                ordered = []
                if pinned and pinned in results:
                    ordered.append(pinned)
                for prod in results.keys():
                    if prod not in ordered:
                        ordered.append(prod)

                for prod in ordered:
                    r = results.get(prod)
                    expanded = prod == st.session_state.get('selected_product') or prod == st.session_state.get('pinned_product')
                    with st.expander(f"Details: {prod}", expanded=bool(expanded)):
                        st.markdown(f"**Fake reviews:** {r.get('fake_count')} of {r.get('n_total')} ({r.get('fake_pct'):.1f}%)")
                        st.json(r.get("trust"))
                        st.subheader("Balanced summary")
                        st.write(r.get("summ", {}).get("balanced_summary"))
                        
                        aspect_scores = r.get("trust", {}).get("aspect_scores", {}) or {}
                        available_aspects = {k: v for k, v in aspect_scores.items() if v is not None}
                        best_aspect = None
                        if available_aspects:
                            try:
                                best_aspect = max(available_aspects.items(), key=lambda kv: float(kv[1]))[0]
                            except Exception:
                                best_aspect = next(iter(available_aspects.keys()))
                            st.markdown(f"**Key aspect (by score):** {best_aspect} â€” {available_aspects.get(best_aspect):.2f}")
            else:
                # Single product detailed view
                cached_results = st.session_state.get('last_results', {})
                if choice and choice in cached_results:
                    r = cached_results[choice]
                    st.markdown(f"### Product Details: {choice}")
                    trust = r.get("trust", {})
                    try:
                        st.metric("Final Trust Score", f"{trust.get('final_score', 0.0):.1f}")
                    except Exception:
                        st.write(f"Final Trust Score: {trust.get('final_score', 0.0):.1f}")
                    
                    st.subheader("Aspect Quality Scores")
                    aspect_scores = trust.get("aspect_scores", {}) or {}
                    available_aspects = {k: v for k, v in aspect_scores.items() if v is not None}
                    
                    if available_aspects:
                        cols = st.columns(2)
                        for idx, (aspect, score) in enumerate(sorted(available_aspects.items(), key=lambda x: x[1], reverse=True)):
                            col_idx = idx % 2
                            with cols[col_idx]:
                                try:
                                    st.metric(aspect.replace('_', ' ').title(), f"{score:.1f}")
                                except Exception:
                                    st.write(f"**{aspect.replace('_', ' ').title()}:** {score:.1f}")
                    else:
                        st.info("No aspect scores available for this product.")
                    
                    st.subheader("Detailed Aspect Breakdown")
                    aspect_breakdown = r.get("summ", {}).get("aspect_breakdown", {}) or {}
                    
                    # Methodology explanation
                    with st.expander("â„¹ï¸ What is the Detailed Aspect Breakdown?", expanded=False):
                        from src.adapters.phone_adapter import ASPECT_KEYWORDS, MIN_MENTIONS_FOR_CONFIDENCE
                        st.markdown("### Component Scoring & Coverage (Personalized)")
                        aspect_counts_local = trust.get('aspect_counts', {}) or {}
                        for aspect in ["battery", "camera", "performance", "display", "build", "software", "connectivity", "value"]:
                            aspect_title = aspect.replace('_', ' ').title()
                            # Use the personalized weights from session state
                            weight = used_weights.get(aspect, 0.0)
                            min_req = MIN_MENTIONS_FOR_CONFIDENCE.get(aspect, 3)
                            kws = ASPECT_KEYWORDS.get(aspect, [])
                            score_val = aspect_scores.get(aspect)
                            mention_ct = aspect_counts_local.get(aspect, 0)
                            status = (f"âœ… Scored: {score_val:.1f}" if score_val is not None else f"âš ï¸ Not scored ({mention_ct}/{min_req} mentions)")
                            line = f"**{aspect_title}** | Weight: {weight:.0%} | Mentions: {mention_ct} (min {min_req}) | {status}\nKeywords: " + ", ".join(kws[:8]) + (" â€¦" if len(kws) > 8 else "")
                            st.markdown(line)
                            
                    # Display aspect details
                    if aspect_breakdown and available_aspects:
                        for aspect in sorted(available_aspects.keys(), key=lambda k: available_aspects[k], reverse=True):
                            aspect_text = aspect_breakdown.get(aspect)
                            if aspect_text:
                                header = f"{aspect.replace('_', ' ').title()} - Score: {available_aspects.get(aspect, 0):.1f}"
                                with st.expander(header):
                                    st.write(aspect_text)
                else:
                    st.warning("Please run 'All (per-product)' analysis first.")
                    if st.button("Analyze this product now"):
                        # Single analysis fallback
                        pass # (Implementation omitted for brevity, user likely won't hit this path with per-product selected)

elif tab == "Analyze YouTube Video":
    st.info("Analyze a video's transcript and fuse with a placeholder text score (mock).")
    vid = st.text_input("YouTube URL or ID", "")
    if st.button("Analyze video") and vid.strip():
        res = analyze_video(vid.strip(), mock_mode=mock_mode)
        if res.get("error"):
            st.error("Transcript not available for this video.")
        else:
            st.write(res)
            text_score = st.slider("Placeholder text review trust score (0-100)", 0, 100, 75)
            fused = fuse_scores(float(text_score), float(res.get("video_score", 0.0)))
            st.metric("Video Score", f"{res.get('video_score', 0.0):.1f}")
            st.metric("Fused Final Score", f"{fused:.1f}")